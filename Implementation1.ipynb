{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":120126,"databundleVersionId":14369730,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Report: Grammar Scoring Engine (v4)\n\n## 1. Approach & Methodology\nThe objective was to predict a grammar score (0-5) from spoken audio. The core hypothesis was that **transcription quality correlates inversely with grammar proficiency**. Models like OpenAI Whisper often \"hallucinate\" fluency by correcting errors, masking the true grammar score. We switched to an acoustic model (Wav2Vec2) to capture the raw, imperfect speech patterns.\n\n## 2. Pipeline Architecture\n\n### A. Preprocessing (The \"Truth\" Transcription)\n* **Replaced Whisper with Wav2Vec2:** Used `facebook/wav2vec2-base-960h`.\n* **Reasoning:** Whisper auto-corrects grammar (e.g., changing \"I has\" to \"I have\"). Wav2Vec2 is an acoustic model that transcribes phonetically, preserving errors and stutters which are critical signals for low scores.\n\n### B. Feature Engineering (Multi-Modal)\nWe extracted features from both the raw audio and the noisy text:\n1.  **Prosody (Fluency):**\n    * **Speech Rate:** Words per second.\n    * **Silence Ratio:** Proportion of non-speech segments (using `librosa`).\n2.  **Grammar Quality (Correctness):**\n    * **Neural GEC Distance:** Calculated Levenshtein distance between the raw text and a corrected version generated by `vennify/t5-base-grammar-correction`. High distance = poor grammar.\n    * **Rule-Based:** Error density using `LanguageTool`.\n3.  **Semantics:**\n    * Generated 384-dim embeddings using `Sentence-BERT` to capture vocabulary and topic coherence.\n\n### C. Modeling\n* **Algorithm:** XGBoost Regressor.\n* **Feature Selection:** Used Recursive Feature Elimination (RFE) to select the top 100 most predictive features from the combined set of explicit grammar features and embeddings.\n* **Validation:** 5-Fold Cross-Validation.\n\n## 3. Results & Evaluation\n* **Metric:** Root Mean Squared Error (RMSE) and Pearson Correlation.\n* **CV Performance:** Average RMSE: **0.6018**, Pearson: **0.6330**.\n* **Leaderboard Score:** **0.766**.","metadata":{}},{"cell_type":"code","source":"!pip install transformers librosa soundfile accelerate pytorch-lightning language-tool-python xgboost textstat sentence-transformers\n!pip install openai-whisper --no-deps\n!pip install tiktoken ffmpeg-python\n!pip install Levenshtein\n!apt-get update\n!apt-get install -y openjdk-17-jdk-headless\n!java -version","metadata":{"execution":{"iopub.status.busy":"2025-12-15T21:10:43.963776Z","iopub.execute_input":"2025-12-15T21:10:43.964118Z","iopub.status.idle":"2025-12-15T21:12:43.479618Z","shell.execute_reply.started":"2025-12-15T21:10:43.964095Z","shell.execute_reply":"2025-12-15T21:12:43.478810Z"},"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport whisper\nimport librosa\nimport Levenshtein\nimport language_tool_python\nimport textstat\nimport xgboost as xgb\nfrom tqdm.notebook import tqdm\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import pearsonr\nimport warnings\n\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T21:12:43.481277Z","iopub.execute_input":"2025-12-15T21:12:43.481558Z","iopub.status.idle":"2025-12-15T21:13:35.124869Z","shell.execute_reply.started":"2025-12-15T21:12:43.481533Z","shell.execute_reply":"2025-12-15T21:13:35.124028Z"}},"outputs":[{"name":"stderr","text":"2025-12-15 21:13:04.385539: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765833184.794905      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765833184.893145      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"# Paths\nBASE_PATH = \"/kaggle/input/shl-intern-hiring-assessment-2025/dataset\"\nTRAIN_AUDIO_DIR = f\"{BASE_PATH}/audios/train\"\nTEST_AUDIO_DIR = f\"{BASE_PATH}/audios/test\"\n\n# Load Whisper\nmodel = whisper.load_model(\"medium.en\")\n\ndef transcribe_data(df, audio_dir):\n    transcriptions = []\n    print(f\"Transcribing {len(df)} files...\")\n    for idx, row in tqdm(df.iterrows(), total=len(df)):\n        filename = row['filename']\n        if not filename.endswith('.wav'): filename += '.wav'\n        path = os.path.join(audio_dir, filename)\n        \n        try:\n            # Beam size 1 helps prevent over-correction of grammar\n            result = model.transcribe(path, beam_size=1, language=\"en\")\n            transcriptions.append(result['text'].strip())\n        except:\n            transcriptions.append(\"\")\n    return transcriptions\n\n# Load Data\ntrain_df = pd.read_csv(f\"{BASE_PATH}/csvs/train.csv\")\ntest_df = pd.read_csv(f\"{BASE_PATH}/csvs/test.csv\")\n\n# Run Transcription (checks if file exists to save time)\nif os.path.exists(\"train_transcribed.csv\"):\n    train_df = pd.read_csv(\"train_transcribed.csv\")\n    test_df = pd.read_csv(\"test_transcribed.csv\")\n    print(\"Loaded saved transcriptions.\")\nelse:\n    train_df['transcription'] = transcribe_data(train_df, TRAIN_AUDIO_DIR)\n    test_df['transcription'] = transcribe_data(test_df, TEST_AUDIO_DIR)\n    train_df.to_csv(\"train_transcribed.csv\", index=False)\n    test_df.to_csv(\"test_transcribed.csv\", index=False)\n\n# Fill NaNs\ntrain_df['transcription'] = train_df['transcription'].fillna(\"\")\ntest_df['transcription'] = test_df['transcription'].fillna(\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T21:13:35.125716Z","iopub.execute_input":"2025-12-15T21:13:35.126578Z","iopub.status.idle":"2025-12-15T22:01:20.603413Z","shell.execute_reply.started":"2025-12-15T21:13:35.126557Z","shell.execute_reply":"2025-12-15T22:01:20.602852Z"}},"outputs":[{"name":"stderr","text":"100%|██████████████████████████████████████| 1.42G/1.42G [00:08<00:00, 178MiB/s]\n","output_type":"stream"},{"name":"stdout","text":"Transcribing 409 files...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/409 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5593e0f8408c4327b4f45d53b75f7552"}},"metadata":{}},{"name":"stdout","text":"Transcribing 197 files...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/197 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6275ba8b38824098b1add4185383c800"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# --- 1. Re-Transcribe with Wav2Vec2 ---\nfrom transformers import pipeline\nimport librosa\nimport os\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\n# Load Wav2Vec2 (Acoustic model, preserves errors)\nasr_dumb = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\", device=0 if torch.cuda.is_available() else -1)\n\ndef transcribe_wav2vec(df, audio_dir):\n    texts = []\n    print(f\"Re-transcribing {len(df)} files in {audio_dir}...\")\n    \n    for _, row in tqdm(df.iterrows(), total=len(df)):\n        fname = row['filename']\n        if not fname.endswith('.wav'): fname += '.wav'\n        path = os.path.join(audio_dir, fname)\n        \n        try:\n            # Wav2Vec2 requires 16kHz audio\n            audio, _ = librosa.load(path, sr=16000)\n            # Transcribe\n            result = asr_dumb(audio, chunk_length_s=30)['text'].lower()\n            texts.append(result)\n        except Exception as e:\n            print(f\"Error {fname}: {e}\")\n            texts.append(\"\")\n            \n    return texts\n\n# 1. Run Transcription\ntrain_df['transcription_raw'] = transcribe_wav2vec(train_df, TRAIN_AUDIO_DIR)\ntest_df['transcription_raw'] = transcribe_wav2vec(test_df, TEST_AUDIO_DIR)\n\n# 2. Swap columns (So your existing code works)\ntrain_df['transcription'] = train_df['transcription_raw']\ntest_df['transcription'] = test_df['transcription_raw']\n\n# 3. Save immediately\ntrain_df.to_csv(\"train_wav2vec.csv\", index=False)\ntest_df.to_csv(\"test_wav2vec.csv\", index=False)\nprint(\"Saved new raw transcriptions!\")","metadata":{"execution":{"iopub.status.busy":"2025-12-15T22:56:12.524899Z","iopub.execute_input":"2025-12-15T22:56:12.525473Z","iopub.status.idle":"2025-12-15T22:59:24.627259Z","shell.execute_reply.started":"2025-12-15T22:56:12.525433Z","shell.execute_reply":"2025-12-15T22:59:24.626613Z"}}},{"cell_type":"code","source":"# --- 3. Advanced Feature Engineering ---\n\n# Initialize Tools\ntool = language_tool_python.LanguageTool('en-US')\ngec_pipe = pipeline(\"text2text-generation\", model=\"vennify/t5-base-grammar-correction\", device=0 if torch.cuda.is_available() else -1)\nembedder = SentenceTransformer('all-MiniLM-L6-v2')\n\ntqdm.pandas() \n\ndef extract_features(row, audio_dir):\n    filename = row['filename']\n    text = row['transcription']\n    features = {}\n\n    # --- 1. PROSODY (Audio) ---\n    path = os.path.join(audio_dir, filename if filename.endswith('.wav') else f\"{filename}.wav\")\n    try:\n        y, sr = librosa.load(path, sr=None)\n        duration = librosa.get_duration(y=y, sr=sr)\n        \n        # Detect non-silent chunks to calculate true speaking time\n        non_silent = librosa.effects.split(y, top_db=20)\n        non_silent_time = sum([end - start for start, end in non_silent]) / sr\n        \n        word_count = len(str(text).split())\n        \n        features['speech_rate'] = word_count / duration if duration > 0 else 0\n        features['articulation_rate'] = word_count / non_silent_time if non_silent_time > 0 else 0\n        features['silence_ratio'] = (duration - non_silent_time) / duration if duration > 0 else 0\n        features['duration'] = duration\n    except:\n        features['speech_rate'] = 0\n        features['articulation_rate'] = 0\n        features['silence_ratio'] = 0\n        features['duration'] = 0\n\n    # --- 2. NEURAL GRAMMAR (GEC Distance) ---\n    try:\n        if len(str(text)) > 1:\n            # Ask T5 to fix the grammar\n            fixed_text = gec_pipe(f\"grammar: {text}\")[0]['generated_text']\n            # Normalized distance: How much did T5 change the original text?\n            features['grammar_distance'] = Levenshtein.distance(text, fixed_text) / len(text)\n        else:\n            features['grammar_distance'] = 0\n    except:\n        features['grammar_distance'] = 0\n\n    # --- 3. RULE-BASED GRAMMAR ---\n    try:\n        matches = tool.check(text)\n        features['error_count'] = len(matches)\n        features['error_density'] = len(matches) / len(text.split()) if len(text.split()) > 0 else 0\n        features['complexity'] = textstat.flesch_kincaid_grade(text)\n    except:\n        features['error_count'] = 0\n        features['error_density'] = 0\n        features['complexity'] = 0\n\n    return pd.Series(features)\n\nprint(\"Extracting Advanced Features (Train)...\")\ntrain_feats = train_df.progress_apply(lambda row: extract_features(row, TRAIN_AUDIO_DIR), axis=1)\n\nprint(\"Extracting Advanced Features (Test)...\")\ntest_feats = test_df.progress_apply(lambda row: extract_features(row, TEST_AUDIO_DIR), axis=1)\n\n# --- 4. EMBEDDINGS (Semantic) ---\nprint(\"Generating Embeddings...\")\nX_train_emb = embedder.encode(train_df['transcription'].fillna(\"\").tolist(), show_progress_bar=True)\nX_test_emb = embedder.encode(test_df['transcription'].fillna(\"\").tolist(), show_progress_bar=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T22:59:36.546371Z","iopub.execute_input":"2025-12-15T22:59:36.547174Z","iopub.status.idle":"2025-12-15T23:23:37.323792Z","shell.execute_reply.started":"2025-12-15T22:59:36.547145Z","shell.execute_reply":"2025-12-15T23:23:37.323142Z"}},"outputs":[{"name":"stderr","text":"WARNING:language_tool_python.server:Unclosed server (server still running at http://127.0.0.1:8317/v2/). Closing it now.\nDevice set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Extracting Advanced Features (Train)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/409 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cadfb1adf829471cb18f3ee4bb696b6f"}},"metadata":{}},{"name":"stdout","text":"Extracting Advanced Features (Test)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/197 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8efd5fa4042140fcbb36b2dabedba1b3"}},"metadata":{}},{"name":"stdout","text":"Generating Embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1d917adc2054cc89c022a91c7212ddf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27c6c9110aca4598a75a9ad3e2433c5a"}},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# Define function to calculate Vocabulary Richness\ndef get_unique_ratio(text):\n    words = str(text).lower().split()\n    if len(words) == 0:\n        return 0\n    return len(set(words)) / len(words)\n\n# Apply to existing dataframes\nprint(\"Adding missing 'unique_ratio' column...\")\ntrain_feats['unique_ratio'] = train_df['transcription'].apply(get_unique_ratio)\ntest_feats['unique_ratio'] = test_df['transcription'].apply(get_unique_ratio)\n\nprint(\"Done! Columns are now:\", train_feats.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T23:23:37.325442Z","iopub.execute_input":"2025-12-15T23:23:37.325766Z","iopub.status.idle":"2025-12-15T23:23:37.340977Z","shell.execute_reply.started":"2025-12-15T23:23:37.325748Z","shell.execute_reply":"2025-12-15T23:23:37.340278Z"}},"outputs":[{"name":"stdout","text":"Adding missing 'unique_ratio' column...\nDone! Columns are now: ['speech_rate', 'articulation_rate', 'silence_ratio', 'duration', 'grammar_distance', 'error_count', 'error_density', 'complexity', 'unique_ratio']\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# Concatenate explicit features + raw embeddings\nX_train_full = pd.concat([train_feats, pd.DataFrame(X_train_emb)], axis=1)\nX_test_full = pd.concat([test_feats, pd.DataFrame(X_test_emb)], axis=1)\n\n# Ensure string column names for XGBoost\nX_train_full.columns = X_train_full.columns.astype(str)\nX_test_full.columns = X_test_full.columns.astype(str)\n\ny_train = train_df['label']\n\n#  FEATURE SELECTION (Recursive Feature Elimination approach)\n# We train a quick model to see what actually matters\nprint(\"Running Feature Selection...\")\nselector_model = xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)\nselector_model.fit(X_train_full, y_train)\n\n# Get feature importances\nimportances = selector_model.feature_importances_\nindices = np.argsort(importances)[::-1] # Sort descending\n\n# Keep top 100 features (Embeddings + specific grammar features)\ntop_n = 100 \ntop_indices = indices[:top_n]\nselected_cols = X_train_full.columns[top_indices]\n\nprint(f\"Selected Top {top_n} features.\")\n\n# Filter datasets\nX_train_selected = X_train_full[selected_cols]\nX_test_selected = X_test_full[selected_cols]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T23:23:37.341611Z","iopub.execute_input":"2025-12-15T23:23:37.341786Z","iopub.status.idle":"2025-12-15T23:23:40.063156Z","shell.execute_reply.started":"2025-12-15T23:23:37.341772Z","shell.execute_reply":"2025-12-15T23:23:40.062321Z"}},"outputs":[{"name":"stdout","text":"Running Feature Selection...\nSelected Top 100 features.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"from scipy.stats import pearsonr\n\n# 3. ROBUST TRAINING (5-Fold CV)\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\noof_preds = np.zeros(len(X_train_selected))\ntest_preds = np.zeros(len(X_test_selected))\nrmse_scores = []\npearson_scores = []  # Track Pearson scores\n\nprint(\"\\nTraining Final Model...\")\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_train_selected, y_train)):\n    X_tr, X_val = X_train_selected.iloc[train_idx], X_train_selected.iloc[val_idx]\n    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n    \n    # Balanced Parameters: Less aggressive regularization than before\n    model = xgb.XGBRegressor(\n        n_estimators=2000,\n        learning_rate=0.02,    # Slightly higher to allow learning\n        max_depth=4,           # Depth 4 captures more interaction than 3\n        subsample=0.8,\n        colsample_bytree=0.8,  # Allow access to more features\n        n_jobs=-1,\n        random_state=42\n    )\n    \n    model.fit(\n        X_tr, y_tr,\n        eval_set=[(X_val, y_val)],\n        early_stopping_rounds=100,\n        verbose=False\n    )\n    \n    # Clip predictions to valid range 0-5\n    val_pred = np.clip(model.predict(X_val), 0, 5)\n    oof_preds[val_idx] = val_pred\n    test_preds += model.predict(X_test_selected) / 5\n    \n    # Calculate metrics\n    rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n    pearson = pearsonr(y_val, val_pred)[0]\n    \n    rmse_scores.append(rmse)\n    pearson_scores.append(pearson)\n    \n    print(f\"Fold {fold+1} | RMSE: {rmse:.4f} | Pearson: {pearson:.4f}\")\n\nprint(f\"\\nAverage CV RMSE: {np.mean(rmse_scores):.4f}\")\nprint(f\"Average CV Pearson: {np.mean(pearson_scores):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T23:27:34.852907Z","iopub.execute_input":"2025-12-15T23:27:34.853229Z","iopub.status.idle":"2025-12-15T23:27:44.002563Z","shell.execute_reply.started":"2025-12-15T23:27:34.853208Z","shell.execute_reply":"2025-12-15T23:27:44.001846Z"}},"outputs":[{"name":"stdout","text":"\nTraining Final Model...\nFold 1 | RMSE: 0.6691 | Pearson: 0.5464\nFold 2 | RMSE: 0.5555 | Pearson: 0.6192\nFold 3 | RMSE: 0.6620 | Pearson: 0.6833\nFold 4 | RMSE: 0.5364 | Pearson: 0.6554\nFold 5 | RMSE: 0.5860 | Pearson: 0.6607\n\nAverage CV RMSE: 0.6018\nAverage CV Pearson: 0.6330\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"final_test_preds = np.clip(test_preds, 0, 5)\n\nsubmission = pd.DataFrame({\n    'filename': test_df['filename'],\n    'label': final_test_preds\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Saved submission.csv\")\nprint(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T23:27:47.281145Z","iopub.execute_input":"2025-12-15T23:27:47.281924Z","iopub.status.idle":"2025-12-15T23:27:47.290112Z","shell.execute_reply.started":"2025-12-15T23:27:47.281899Z","shell.execute_reply":"2025-12-15T23:27:47.289487Z"}},"outputs":[{"name":"stdout","text":"Saved submission.csv\n    filename     label\n0  audio_141  2.819240\n1  audio_114  3.113828\n2   audio_17  2.865006\n3   audio_76  4.273753\n4  audio_156  2.971206\n","output_type":"stream"}],"execution_count":25}]}